import streamlit as st
import requests
import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import OPENWEATHER_API_KEY, TRANSFORMERS_MODEL_NAME, MODEL_SUPPORTS_TOOLS

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Transformers ---
DEFAULT_TEMPERATURE = 0.2
MAX_NEW_TOKENS = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–≥–æ–¥—ã ---
def get_weather(city: str) -> str:
    try:
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ru"
        response = requests.get(url)
        data = response.json()

        if response.status_code == 200:
            temp = data["main"]["temp"]
            feels_like = data["main"]["feels_like"]
            description = data["weather"][0]["description"]
            humidity = data["main"]["humidity"]
            return f"–í –≥–æ—Ä–æ–¥–µ {city} —Å–µ–π—á–∞—Å {temp:.1f}¬∞C, –æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫ {feels_like:.1f}¬∞C. {description.capitalize()}. –í–ª–∞–∂–Ω–æ—Å—Ç—å: {humidity}%."
        else:
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–≥–æ–¥—É –¥–ª—è {city}. –û—à–∏–±–∫–∞: {data.get('message', 'Unknown error')}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –ø–æ–≥–æ–¥—ã: {str(e)}"

# --- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (–≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ—Ö–æ–∂–µ–º –Ω–∞ OpenAI) ---
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–æ–≥–æ–¥—É –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≥–æ—Ä–æ–¥–µ",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ú–æ—Å–∫–≤–∞, Berlin, Tokyo"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Transformers ---
@st.cache_resource(show_spinner=True)
def _load_model_and_tokenizer(model_name: str):
    tok = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=DTYPE,
        device_map="auto" if DEVICE == "cuda" else None,
    )
    if DEVICE != "cuda":
        model = model.to(DEVICE)
    return tok, model


def _build_system_prompt(tools_spec, supports_tools: bool) -> str:
    base = (
        "–¢—ã ‚Äî —É–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
    )
    if supports_tools and tools_spec:
        tool_desc = (
            "–¢—ã —É–º–µ–µ—à—å –≤—ã–∑—ã–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ (tools). –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –ø–æ–≥–æ–¥–æ–π, –≤—ã–∑–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏—é get_weather.\n"
            "–°—Ö–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: " + json.dumps(tools_spec, ensure_ascii=False) + "\n\n"
            "–ï—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–∑–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –≤–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ JSON –≤–∏–¥–∞:\n"
            "{\"tool_calls\":[{\"id\":\"toolcall_1\",\"function\":{\"name\":\"get_weather\",\"arguments\":\"{\\\"city\\\":\\\"–ú–æ—Å–∫–≤–∞\\\"}\"}}]}\n"
            "–ë–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø–æ—è—Å–Ω–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥. –ï—Å–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –Ω–µ –Ω—É–∂–µ–Ω, –æ—Ç–≤–µ—Ç—å –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º."
        )
        return base + "\n\n" + tool_desc
    return base


def _apply_chat_template_or_concat(tok: AutoTokenizer, messages: list, system_prompt: str) -> str:
    # Try to use chat template if available
    msgs = []
    inserted_system = False
    for m in messages:
        if m["role"] == "system":
            inserted_system = True
    if not inserted_system and system_prompt:
        msgs.append({"role": "system", "content": system_prompt})
    for m in messages:
        msgs.append(m)
    try:
        return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    except Exception:
        # Fallback: simple concatenation
        parts = []
        if system_prompt:
            parts.append(f"[–°–ò–°–¢–ï–ú–ê]\n{system_prompt}\n")
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            if role in ("user", "assistant"):
                parts.append(f"[{role.upper()}]\n{content}\n")
        parts.append("[ASSISTANT]\n")
        return "\n".join(parts)


def _maybe_extract_tool_calls(text: str):
    # Try to find a JSON block containing "tool_calls"
    try:
        # Direct JSON
        data = json.loads(text)
        if isinstance(data, dict) and "tool_calls" in data:
            return data.get("tool_calls")
    except Exception:
        pass
    # Regex search for {..."tool_calls": ...}
    match = re.search(r"\{[^\{\}]*\"tool_calls\"\s*:\s*\[[\s\S]*?\]\s*\}", text)
    if match:
        try:
            data = json.loads(match.group(0))
            return data.get("tool_calls")
        except Exception:
            return None
    return None

def _generate_assistant_message(messages, tools_spec=None, supports_tools: bool = True):
    tok, model = _load_model_and_tokenizer(TRANSFORMERS_MODEL_NAME)
    system_prompt = _build_system_prompt(tools_spec, supports_tools)
    prompt_text = _apply_chat_template_or_concat(tok, messages, system_prompt)

    inputs = tok(prompt_text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=DEFAULT_TEMPERATURE,
            pad_token_id=tok.eos_token_id,
            eos_token_id=tok.eos_token_id,
        )
    full_text = tok.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant continuation if chat template didn't do it
    # Heuristic: take the tail after the prompt_text
    if full_text.startswith(prompt_text):
        assistant_text = full_text[len(prompt_text):].strip()
    else:
        assistant_text = full_text.strip()

    # Try detect tool calls
    tool_calls = _maybe_extract_tool_calls(assistant_text) if supports_tools else None

    if tool_calls:
        return {"role": "assistant", "content": "", "tool_calls": tool_calls}
    else:
        return {"role": "assistant", "content": assistant_text}

# --- Streamlit UI ---
st.set_page_config(page_title="üå§Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ–≥–æ–¥—ã (Transformers)", page_icon="üå§Ô∏è")
st.title("üå§Ô∏è –õ–æ–∫–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ–≥–æ–¥—ã")
st.markdown(f"–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–æ–¥–µ–ª–∏ Transformers: **{TRANSFORMERS_MODEL_NAME}**. –õ–æ–∫–∞–ª—å–Ω–æ, –±–µ–∑ –æ–±–ª–∞–∫–∞!")

# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ tools –≤ –º–æ–¥–µ–ª–∏ ---
if MODEL_SUPPORTS_TOOLS:
    st.info("üß∞ –†–µ–∂–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤–∫–ª—é—á—ë–Ω: –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –≤—ã–∑—ã–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏.")
else:
    st.warning("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: –æ—Ç–≤–µ—Ç—ã –±—É–¥—É—Ç –±–µ–∑ tool-calling.")

# --- –ö–ª—é—á –æ—Ç OpenWeather ---
# –†–∞–Ω–µ–µ: OPENWEATHER_API_KEY = st.secrets["OPENWEATHER_API_KEY"]

# --- –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system",
            "content": (
                "–¢—ã ‚Äî —É–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã –º–æ–∂–µ—à—å –≤—ã–∑—ã–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏. "
                "–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –ø–æ–≥–æ–¥–µ, –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç get_weather. "
                "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
            )
        }
    ]

# --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ ---
for msg in st.session_state.messages:
    if msg["role"] in ["user", "assistant"]:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

# --- –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
if prompt := st.chat_input("–°–ø—Ä–æ—Å–∏—Ç–µ –æ –ø–æ–≥–æ–¥–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä: '–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ü–∞—Ä–∏–∂–µ?'"):
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Transformers
    with st.spinner("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥—É–º–∞–µ—Ç..."):
        response_msg = _generate_assistant_message(
            st.session_state.messages,
            tools_spec=tools,
            supports_tools=MODEL_SUPPORTS_TOOLS,
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏
        tool_calls = response_msg.get("tool_calls") if isinstance(response_msg, dict) else None
        if MODEL_SUPPORTS_TOOLS and tool_calls:
            st.session_state.messages.append(response_msg)

            for tool_call in tool_calls:
                function_name = tool_call.get("function", {}).get("name")
                if function_name == "get_weather":
                    try:
                        args_raw = tool_call.get("function", {}).get("arguments", "{}")
                        # arguments –º–æ–∂–µ—Ç –±—ã—Ç—å JSON-—Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —É–∂–µ dict
                        if isinstance(args_raw, str):
                            args = json.loads(args_raw)
                        else:
                            args = args_raw or {}
                        city = args.get("city", "")
                        result = get_weather(city)

                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
                        st.session_state.messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.get("id", "toolcall_1"),
                            "name": function_name,
                            "content": result,
                        })
                    except Exception as e:
                        st.session_state.messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.get("id", "toolcall_1"),
                            "name": function_name,
                            "content": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤: {e}",
                        })

            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            final_response = _generate_assistant_message(
                st.session_state.messages,
                tools_spec=None,
                supports_tools=False,
            )
            st.session_state.messages.append(final_response)
            with st.chat_message("assistant"):
                st.write(final_response.get("content", ""))
        else:
            # –ü—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –±–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            st.session_state.messages.append(response_msg)
            with st.chat_message("assistant"):
                st.write(response_msg.get("content", ""))
